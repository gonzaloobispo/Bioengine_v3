import os
import json
import sqlite3
import datetime
import time
import re
import asyncio
import logging
from google import genai
from google.genai import types
from typing import Optional, List, Dict, Any, Union
from services.context_manager import ContextManager
from services.multi_model_client import MultiModelClient
from services.cost_control import CostControl
from models.schemas import ActivitySchema, BodyCompositionSchema
from models.schemas_biomecanica import GaitAnalysis, TennisFatigue, AthleteBiometrics2026, RiskAssessment
from services.biomechanics_pipeline import BiomechanicsPipeline
from pydantic import ValidationError
from services.mcp.mcp_client import MCPClient
from services.agents.agent_registry import AgentRegistry
from services.agents.router_agent import RouterAgent
from services.firebase_service import FirebaseService
from services.agents.coach_agent import CoachAgent
from services.agents.recovery_agent import RecoveryAgent
from services.agents.biomechanics_agent import BiomechanicsAgent
from services.agents.skills.notebooklm_bridge.bridge_logic import NotebookLMBridge

from config import DB_PATH, LOG_FILE, GEMINI_MODEL

# Setup detailed logging for debugging
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AIService:
    # === PAUSE FLAG: Set to False to disable AI API calls ===
    AI_ENABLED = True  # <-- Change to True when ready to reactivate AI
    
    def __init__(self):
        self.db_path = DB_PATH
        # Using configured gemini model
        self.model_name = GEMINI_MODEL
        self.client = None
        self.context_manager = ContextManager()
        self._analysis_cache = {"timestamp": 0, "content": None}
        self._lock = None
        self._message_count = 0
        # Inicializar Infraestructura Multi-Agente (SOTA 2026)
        self.agent_registry = AgentRegistry()
        self.mcp_client = MCPClient()
        
        # Registrar especialistas (se completar√° el setup del cliente tras _setup_gemini)
        self.agent_registry.register(CoachAgent(self.mcp_client))
        self.agent_registry.register(RecoveryAgent(self.mcp_client))
        self.agent_registry.register(BiomechanicsAgent(self.mcp_client))
        
        # Inicializar Router
        self.router = RouterAgent(self.agent_registry)
        self.firebase = FirebaseService()
        
        # NotebookLM Bridge Implementation
        self.notebooklm_bridge = NotebookLMBridge(self.mcp_client)

        # Only initialize AI clients if enabled
        if self.AI_ENABLED:
            self._setup_gemini()
            self._setup_multi_model_client()
        else:
            logger.info("AI APIs are PAUSED. Set AI_ENABLED = True to reactivate.")

    async def is_notebooklm_ready(self) -> bool:
        """Verifica si el gateway de NotebookLM (Context MCP) responde."""
        try:
            # Intentamos una peque√±a consulta al contexto
            ctx = await self.mcp_client.get_full_coach_context()
            return ctx is not None and len(ctx) > 0
        except Exception:
            return False

    def _get_connection(self) -> sqlite3.Connection:
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _get_gemini_key(self) -> Optional[str]:
        # BioEngine V4: Prioritize environment variables from .env
        from config import GEMINI_API_KEY
        if GEMINI_API_KEY:
            logger.info("Using Gemini API key from environment/config")
            return GEMINI_API_KEY.strip()

        # Fallback to database (legacy)
        conn = self._get_connection()
        row = conn.execute("SELECT credentials_json FROM secrets WHERE service = ?", ('gemini',)).fetchone()
        conn.close()
        if not row:
            return None
            
        try:
            val = row['credentials_json']
            # If it's already a dict
            if isinstance(val, dict):
                data = val
            else:
                data = json.loads(val)
                
            if isinstance(data, dict):
                # Try common keys
                for key in ['GEMINI_API_KEY', 'api_key', 'key']:
                    if key in data:
                        return str(data[key]).strip()
                return None
            return str(data).strip()
        except Exception:
            # Fallback to raw string if not JSON
            return str(row['credentials_json']).strip()

    def _setup_gemini(self):
        self.api_key = self._get_gemini_key()
        if not self.api_key:
            logger.warning("No Gemini API key found in secrets")
            print("Warning: No Gemini API key found")
            self.client = None
        else:
            self.client = genai.Client(api_key=self.api_key)
            if self.client:
                logger.info("Gemini API client initialized successfully")
                # Inyectar cliente y modelo en los agentes registrados
                for agent in self.agent_registry.get_all().values():
                    agent.model_client = self.client
                    agent._model_name = self.model_name
                self.router.model_client = self.client
    
    def _get_all_api_keys(self):
        """Retrieve all API keys for multi-model client."""
        conn = self._get_connection()
        keys = {}
        try:
            rows = conn.execute("SELECT provider, api_key FROM api_keys WHERE enabled = 1 ORDER BY priority ASC").fetchall()
            for row in rows:
                keys[row['provider']] = row['api_key']
        except sqlite3.OperationalError as e:
            logger.warning(f"Could not load API keys for multi-model: {e}")
        finally:
            conn.close()
        return keys
    
    def _setup_multi_model_client(self):
        """Initialize the multi-model client with fallback capability."""
        try:
            api_keys = self._get_all_api_keys()
            if api_keys:
                cost_control = CostControl()
                self.multi_model_client = MultiModelClient(api_keys, cost_control)
                logger.info(f"Multi-model client initialized with {len(api_keys)} providers")
            else:
                logger.warning("No API keys found, multi-model client not initialized")
        except Exception as e:
            logger.error(f"Failed to initialize multi-model client: {e}")
            self.multi_model_client = None

    def _get_user_context(self) -> str:
        conn = self._get_connection()
        try:
            # Reduce context size to save tokens and avoid hitting rate limits faster
            raw_activities = conn.execute("SELECT * FROM activities ORDER BY fecha DESC LIMIT 5").fetchall()
            raw_biometrics = conn.execute("SELECT * FROM biometrics ORDER BY fecha DESC LIMIT 3").fetchall()
            
            activities: List[ActivitySchema] = []
            for row in raw_activities:
                try:
                    activities.append(ActivitySchema(**dict(row)))
                except Exception:
                    continue

            biometrics: List[BodyCompositionSchema] = []
            for row in raw_biometrics:
                try:
                    biometrics.append(BodyCompositionSchema(**dict(row)))
                except Exception:
                    continue

            context = "CONTEXTO DEL USUARIO (BIOENGINE V3):\n"
            context += "√öltimas Actividades:\n"
            for a in activities:
                date_str = a.fecha.strftime('%Y-%m-%d') if hasattr(a.fecha, 'strftime') else str(a.fecha)
                context += f"- {date_str}: {a.tipo}, {a.distancia_km}km, {a.duracion_min}min, {a.calorias}cal\n"
            
            context += "\n√öltima Biometr√≠a (Peso):\n"
            for b in biometrics:
                date_b = b.fecha.strftime('%Y-%m-%d') if hasattr(b.fecha, 'strftime') else str(b.fecha)
                context += f"- {date_b}: {b.peso}kg, {b.grasa_pct}% grasa\n"
        finally:
            conn.close()
            
        return context

    async def _generate_content_with_retry(self, prompt: str, system_instruction: Optional[str] = None, retries: int = 3) -> str:
        """Helper to call Gemini API via SDK with retry logic for 429 errors."""
        if not self.client:
            self._setup_gemini()
            if not self.client:
                raise Exception("Gemini client not initialized")

        logger.info(f"Starting API call with model: {self.model_name}")
        
        config = None
        if system_instruction:
            config = types.GenerateContentConfig(system_instruction=system_instruction)

        for attempt in range(retries):
            try:
                logger.info(f"Attempt {attempt + 1}/{retries}")
                response = await self.client.aio.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=config
                )
                
                if response and response.text:
                    logger.info("API call successful via SDK")
                    return response.text
                else:
                    raise Exception("Empty response from Gemini SDK")
                    
            except Exception as e:
                err_msg = str(e)
                logger.warning(f"Gemini SDK error (attempt {attempt+1}): {err_msg}")
                
                if "429" in err_msg or "RESOURCE_EXHAUSTED" in err_msg:
                    delay = 10 * (attempt + 1)
                    logger.warning(f"Rate limit hit, waiting {delay}s")
                    print(f"Gemini API Quota Exceeded. Waiting {delay:.1f}s before retry {attempt+1}/{retries}...")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Non-retryable SDK error: {err_msg}")
                    raise e
        
        logger.error("Failed after max retries")
        raise Exception("Failed after max retries due to rate limiting or other errors.")

    def _format_chat_history(self, chat_history, limit=12):
        if not chat_history:
            return ""

        lines = []
        for msg in chat_history[-limit:]:
            if not isinstance(msg, dict):
                continue
            role = (msg.get("role") or msg.get("sender") or msg.get("type") or "").lower()
            text = msg.get("text") or msg.get("content") or msg.get("message")
            if not text:
                continue
            role_label = "Usuario" if role in {"user", "usuario", "human"} else "Coach"
            lines.append(f"{role_label}: {text}")

        if not lines:
            return ""

        return "=== HISTORIAL DE CHAT ===\n" + "\n".join(lines)

    async def get_response(self, user_message: str, chat_history: Optional[List[dict]] = None):
        """
        BioEngine V4: Usa el RouterAgent para derivar la consulta al especialista adecuado.
        """
        logger.info(f"AI Search query: {user_message}")
        
        # 1. Obtener contexto v√≠a MCP (Standard V4)
        context = await self.mcp_client.get_full_coach_context()
        
        # 2. Despachar v√≠a Router
        if not self.router:
            # Fallback a l√≥gica de V3 si el router no est√° inicializado (no deber√≠a pasar)
            logger.warning("RouterAgent no inicializado. Usando fallback.")
            return await self._get_managed_response(user_message, chat_history)
            
        agent_response = await self.router.route(user_message, context, chat_history)
        
        # 3. Formatear respuesta final del agente
        if "error" in agent_response:
            return f"Hubo un error procesando tu consulta con el {agent_response['agent']}: {agent_response['error']}"
            
        final_text = agent_response.get("response", "No se gener√≥ respuesta.")
        
        # A√±adir marca de agua de enrutamiento
        selected = agent_response["_router"]["selected_agent"]
        final_text += f"\n\n---\n*Respuesta generada por: Specialized {selected.capitalize()} Agent (BioEngine V4)*"

        # Sincronizar con BioConnect iOS via Firebase
        try:
            await self.firebase.sync_agent_response(selected, agent_response)
        except Exception as e:
            logger.error(f"Error syncing agent response with Firebase: {e}")
        
        return final_text

    async def _get_managed_response(self, user_message: str, chat_history: Optional[List[dict]] = None):
        # Mantiene la l√≥gica original de get_response pero como m√©todo interno
        if not self.AI_ENABLED:
            return "El asistente de IA est√° temporalmente pausado. Tus datos de actividades y biom√©tricos siguen sincroniz√°ndose normalmente. El an√°lisis se reactivar√° pronto."
        
        if chat_history is None:
            chat_history = []
        system_instruction = (
            "Eres BioEngine Coach, un asistente experto en triatl√≥n, running (calle y trail), tenis y salud biomec√°nica.\n"
            f"FECHA ACTUAL: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n"
            "=== MEMORIA Y CONTEXTO BASE ===\n"
            f"{self.context_manager.get_foundational_context()}\n\n"
            "=== INSTRUCCIONES DE ESPECIALIDAD ===\n"
            "1. RUNNING & TENIS: Tus consejos deben optimizar el rendimiento en carrera de calle/trail y la agilidad en tenis master.\n"
            "2. SALUD BIOMEC√ÅNICA: Prioriza la protecci√≥n de articulaciones (espec√≠ficamente la rodilla derecha) mediante ejercicios de fortalecimiento y movilidad.\n"
            "3. ADHERENCIA Y H√ÅBITOS: Utiliza t√©cnicas de psicolog√≠a deportiva para fomentar la constancia. Si el usuario muestra desmotivaci√≥n, ajusta el plan o refuerza los hitos logrados.\n"
            "4. GENERACI√ìN DE PLANES Y EJERCICIOS: Al proponer un plan o ejercicios espec√≠ficos:\n"
            "   - Usa TABLAS MARKDOWN o LISTAS NUMERADAS para organizar series, repeticiones y descansos.\n"
            "   - Incluye una GU√çA DE EJECUCI√ìN (2-3 l√≠neas) para cada ejercicio, explicando la t√©cnica correcta y puntos clave de seguridad.\n"
            "   - Especifica el PROP√ìSITO BIOMEC√ÅNICO de cada ejercicio (ej: 'Poliquin Step-Up ‚Üí Fortalece vasto medial ‚Üí Protege rodilla en descensos').\n"
            "   - Formato ejemplo:\n"
            "     **Ejercicio 1: Sentadilla B√∫lgara**\n"
            "     - Series: 3 x 12 reps por pierna\n"
            "     - Descanso: 90 segundos\n"
            "     - Ejecuci√≥n: Pie trasero elevado, rodilla delantera alineada con tobillo, descenso controlado.\n"
            "     - Prop√≥sito: Fortalecimiento unilateral del cu√°driceps y gl√∫teo, mejora estabilidad de rodilla.\n\n"
            "Tus respuestas deben ser precisas, motivadoras pero realistas, y basadas tanto en los datos hist√≥ricos como en el conocimiento base.\n"
            "IMPORTANTE: Ten en cuenta la l√≠nea de tiempo y las restricciones de lesiones activas.\n\n"
            "=== AUTO-ACTUALIZACI√ìN DE MEMORIA ===\n"
            "Si el usuario informa dolor f√≠sico: [COMMAND: LOG_PAIN: nivel]\n"
            "Si el usuario confirma que complet√≥ un entrenamiento: [COMMAND: UPDATE_CONTEXT: se complet√≥ X ejercicio].\n"
            "Habla en espa√±ol de forma natural y profesional."
        )

        full_context = self._get_user_context()
        history_block = self._format_chat_history(chat_history)
        prompt_parts = [full_context]
        if history_block:
            prompt_parts.append(history_block)
        prompt_parts.append(f"Usuario: {user_message}")
        prompt = "\n\n".join(prompt_parts)

        response = None

        if self.multi_model_client is None:
            self._setup_multi_model_client()

        if self.multi_model_client:
            try:
                logger.info("Attempting chat with multi-model client...")
                response = self.multi_model_client.generate(
                    prompt=prompt,
                    system_instruction=system_instruction,
                    max_tokens=1200
                )
            except Exception as fallback_error:
                logger.error(f"Chat multi-model failed: {fallback_error}")

        if response is None:
            if not self.api_key:
                self._setup_gemini()

            if self.api_key:
                try:
                    response = await self._generate_content_with_retry(prompt, system_instruction=system_instruction)
                except Exception as e:
                    logger.error(f"Chat Error with Gemini: {e}")

        if response is None:
            if self.multi_model_client is None and not self.api_key:
                return "Error: API keys no encontradas para generar respuesta."
            return "Error generando respuesta: todos los modelos alcanzaron sus l√≠mites o no est√°n configurados."

        processed_response = response
        if "[COMMAND:" in response:
            pain_match = re.search(r"\[COMMAND: LOG_PAIN: (\d+)\]", response)
            if pain_match:
                level = int(pain_match.group(1))
                self.context_manager.log_pain(level, f"Registrado v√≠a chat: {user_message[:100]}")
                logger.info(f"Pain logged from AI response: {level}")

            update_match = re.search(r"\[COMMAND: UPDATE_CONTEXT: (.+?)\]", response)
            if update_match:
                update_text = update_match.group(1).strip()
                self.context_manager.log_context_update(update_text, source="chat")
                logger.info("Context update logged from AI response")
                try:
                    asyncio.create_task(self._update_semantic_summary())
                except RuntimeError:
                    await self._update_semantic_summary()

            processed_response = re.sub(r"\[COMMAND:.*?\]", "", response).strip()

        self._message_count += 1
        if self._message_count % self._semantic_refresh_every == 0:
            try:
                asyncio.create_task(self._update_semantic_summary(force=True))
            except RuntimeError:
                await self._update_semantic_summary(force=True)

        return processed_response

    async def get_streaming_response(self, user_message: str, chat_history: Optional[List[dict]] = None):
        """
        Generador as√≠ncrono para Streaming SSE.
        Yields: Chunks de texto.
        Post-procesamiento: Ejecuta comandos al finalizar el stream.
        """
        if not self.AI_ENABLED:
            yield "El asistente de IA est√° temporalmente pausado. Tus datos de actividades y biom√©tricos siguen sincroniz√°ndose normalmente. El an√°lisis se reactivar√° pronto."
            return

        if chat_history is None:
            chat_history = []
        
        # Reutilizar l√≥gica de construcci√≥n de prompt (podr√≠amos refactorizar esto a un m√©todo privado com√∫n)
        system_instruction = (
            "Eres BioEngine Coach, un asistente experto en triatl√≥n, running (calle y trail), tenis y salud biomec√°nica.\n"
            f"FECHA ACTUAL: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n"
            "=== MEMORIA Y CONTEXTO BASE ===\n"
            f"{self.context_manager.get_foundational_context()}\n\n"
            "=== INSTRUCCIONES DE ESPECIALIDAD ===\n"
            "1. RUNNING & TENIS: Tus consejos deben optimizar el rendimiento en carrera de calle/trail y la agilidad en tenis master.\n"
            "2. SALUD BIOMEC√ÅNICA: Prioriza la protecci√≥n de articulaciones (espec√≠ficamente la rodilla derecha) mediante ejercicios de fortalecimiento y movilidad.\n"
            "3. ADHERENCIA Y H√ÅBITOS: Utiliza t√©cnicas de psicolog√≠a deportiva para fomentar la constancia. Si el usuario muestra desmotivaci√≥n, ajusta el plan o refuerza los hitos logrados.\n"
            "4. GENERACI√ìN DE PLANES: Tienes la capacidad de proponer micro-sesiones de ejercicio adaptadas a la etapa f√≠sica actual del usuario (registrada en su historial m√©dico y de dolor).\n\n"
            "Tus respuestas deben ser precisas, motivadoras pero realistas, y basadas tanto en los datos hist√≥ricos como en el conocimiento base.\n"
            "IMPORTANTE: Ten en cuenta la l√≠nea de tiempo y las restricciones de lesiones activas.\n\n"
            "=== AUTO-ACTUALIZACI√ìN DE MEMORIA ===\n"
            "Si el usuario informa dolor f√≠sico: [COMMAND: LOG_PAIN: nivel]\n"
            "Si el usuario confirma que complet√≥ un entrenamiento: [COMMAND: UPDATE_CONTEXT: se complet√≥ X ejercicio].\n"
            "Habla en espa√±ol de forma natural y profesional."
        )

        full_context = self._get_user_context()
        history_block = self._format_chat_history(chat_history)
        prompt_parts = [full_context]
        if history_block:
            prompt_parts.append(history_block)
        prompt_parts.append(f"Usuario: {user_message}")
        prompt = "\n\n".join(prompt_parts)

        if not self.multi_model_client:
             self._setup_multi_model_client()

        full_response_accumulator = ""
        
        try:
            logger.info("Starting Multi-Model Streaming API call")
            async for chunk in self.multi_model_client.generate_stream(
                prompt=prompt,
                system_instruction=system_instruction
            ):
                full_response_accumulator += chunk
                yield chunk

        except Exception as e:
            logger.error(f"Multi-Model Streaming Error: {e}")
            yield f"\n[Error cr√≠tico del sistema: {str(e)}]"
            return

        # --- Post-procesamiento de Comandos (Invisible para el yield, pero ejecuta l√≥gica) ---
        if "[COMMAND:" in full_response_accumulator:
            pain_match = re.search(r"\[COMMAND: LOG_PAIN: (\d+)\]", full_response_accumulator)
            if pain_match:
                level = int(pain_match.group(1))
                self.context_manager.log_pain(level, f"Registrado v√≠a chat (Stream): {user_message[:100]}")
                logger.info(f"Pain logged from AI response (Stream): {level}")

            update_match = re.search(r"\[COMMAND: UPDATE_CONTEXT: (.+?)\]", full_response_accumulator)
            if update_match:
                update_text = update_match.group(1).strip()
                self.context_manager.log_context_update(update_text, source="chat_stream")
                logger.info("Context update logged from AI response (Stream)")
                try:
                    asyncio.create_task(self._update_semantic_summary())
                except RuntimeError:
                    await self._update_semantic_summary()

        # Actualizar contadores
        self._message_count += 1
        if self._message_count % self._semantic_refresh_every == 0:
             try:
                asyncio.create_task(self._update_semantic_summary(force=True))
             except RuntimeError:
                await self._update_semantic_summary(force=True)

    async def _update_semantic_summary(self, force=False):
        data = self.context_manager.get_semantic_summary_data()
        total_count = data.get("total_count", 0)
        
        if total_count == 0:
            return

        last_count = data.get("last_count", 0)
        # Si force=True o hay m√°s entradas que la √∫ltima vez
        if force or total_count > last_count:
            logger.info(f"Updating semantic summary. New entries: {total_count - last_count}")
            
            # Obtener solo las nuevas memorias desde la base de datos
            new_memories = self.context_manager.get_new_evolutionary_memories(last_count)
            
            if not new_memories:
                return

            new_text = "\n".join([f"- [{m['date']}] {m['lesson']} ({m['context']})" for m in new_memories])
            
            system_instruction = (
                "Eres un sistema de memoria de BioEngine. Resume en espa√±ol la memoria evolutiva del usuario. "
                "Debes producir un resumen compacto pero rico: perfil, lesiones activas, restricciones, objetivos, "
                "preferencias, h√°bitos clave y cualquier patr√≥n importante. Mant√©n el tono cl√≠nico-profesional. "
                "Salida: 6-10 l√≠neas concisas, sin listas numeradas ni emojis."
            )

            current_summary = data.get("current_summary", "")
            prompt = f"RESUMEN ACTUAL:\n{current_summary}\n\nNUEVAS ENTRADAS:\n{new_text}\n\nGenera el resumen actualizado:"

            summary_text = None
            if self.multi_model_client is None:
                self._setup_multi_model_client()

            if self.multi_model_client:
                try:
                    summary_text = self.multi_model_client.generate(
                        prompt=prompt,
                        system_instruction=system_instruction,
                        max_tokens=600
                    )
                except Exception as e:
                    logger.error(f"Semantic summary via multi-model failed: {e}")

            if summary_text:
                self.context_manager.set_semantic_summary(summary_text, total_count)



    async def get_coach_analysis(self) -> str:
        # Return static message if AI is paused
        if not self.AI_ENABLED:
            return """üìä **An√°lisis del Coach - MODO OFFLINE**

El an√°lisis de IA est√° temporalmente pausado mientras se resuelven l√≠mites de cuota de la API.

**Mientras tanto, puedes:**
‚Ä¢ Sincronizar datos de Garmin y Withings normalmente
‚Ä¢ Revisar tus actividades y m√©tricas en el dashboard
‚Ä¢ Consultar el historial de peso y biometr√≠a

*El an√°lisis inteligente se reactivar√° pronto.*"""
        
        # Initialize lock lazily to ensure it attaches to the current event loop
        if self._lock is None:
            self._lock = asyncio.Lock()

        async with self._lock:
            # Check cache with dynamic TTL
            now = time.time()
            if self._analysis_cache["content"]:
                ttl = self._analysis_cache.get("ttl", 900)
                if now - self._analysis_cache["timestamp"] < ttl:
                    return self._analysis_cache["content"]

            if not self.api_key:
                self._setup_gemini()
                if not self.api_key:
                    return "Configura tu API Key para ver el an√°lisis."

            # Get enhanced context with more data points
            conn = self._get_connection()
            try:
                # Get last 50 activities for better trend analysis (including synced competitions)
                raw_activities = conn.execute("SELECT * FROM activities ORDER BY fecha DESC LIMIT 50").fetchall()
                # Get last 5 weight measurements for trend
                raw_biometrics = conn.execute("SELECT * FROM biometrics ORDER BY fecha DESC LIMIT 5").fetchall()

                activities: List[ActivitySchema] = []
                for row in raw_activities:
                    try:
                        activities.append(ActivitySchema(**dict(row)))
                    except ValidationError as e:
                        logger.warning(f"Skipping invalid activity {row['id']}: {e}")

                biometrics: List[BodyCompositionSchema] = []
                for row in raw_biometrics:
                    try:
                        biometrics.append(BodyCompositionSchema(**dict(row)))
                    except ValidationError as e:
                        logger.warning(f"Skipping invalid biometric {row['id']}: {e}")
                
                # Build detailed context
                context = "DATOS DEL ATLETA (Gonzalo - 49 a√±os, Tenis Master):\n\n"
                
                # Activity summary
                context += "üìä ACTIVIDADES RECIENTES:\n"
                if activities:
                    total_km = sum(a.distancia_km or 0 for a in activities)
                    total_time = sum(a.duracion_min or 0 for a in activities)
                    activity_types = {}
                    for a in activities:
                        tipo = a.tipo or 'Desconocido'
                        activity_types[tipo] = activity_types.get(tipo, 0) + 1
                        # Note: fecha is datetime object now if parsed correctly, or str if schema keeps it str. 
                        # We defined datetime in schema, so let's format it.
                        date_str = a.fecha.strftime('%Y-%m-%d') if hasattr(a.fecha, 'strftime') else str(a.fecha)
                        context += f"  ‚Ä¢ {date_str}: {tipo} - {a.distancia_km or 0}km, {a.duracion_min or 0}min, {a.calorias or 0}cal\n"
                    
                    context += f"\nRESUMEN: {len(activities)} actividades, {total_km:.1f}km totales, {total_time:.0f}min\n"
                    context += f"Tipos: {', '.join([f'{k} ({v})' for k, v in activity_types.items()])}\n"
                else:
                    context += "  No hay actividades registradas recientemente.\n"
                
                # Weight trend
                context += "\n‚öñÔ∏è TENDENCIA DE PESO:\n"
                if biometrics and len(biometrics) >= 2:
                    latest = biometrics[0]
                    oldest = biometrics[-1]
                    diff = latest.peso - oldest.peso
                    date_latest = latest.fecha if isinstance(latest.fecha, str) else latest.fecha.strftime('%Y-%m-%d')
                    date_oldest = oldest.fecha if isinstance(oldest.fecha, str) else oldest.fecha.strftime('%Y-%m-%d')
                    
                    context += f"  ‚Ä¢ Actual: {latest.peso}kg ({date_latest})\n"
                    context += f"  ‚Ä¢ Anterior: {oldest.peso}kg ({date_oldest})\n"
                    context += f"  ‚Ä¢ Cambio: {diff:+.2f}kg\n"
                    if latest.grasa_pct:
                        context += f"  ‚Ä¢ Grasa corporal: {latest.grasa_pct}%\n"
                elif biometrics:
                    b = biometrics[0]
                    date_b = b.fecha if isinstance(b.fecha, str) else b.fecha.strftime('%Y-%m-%d')
                    context += f"  ‚Ä¢ Peso actual: {b.peso}kg ({date_b})\n"
                else:
                    context += "  No hay datos de peso disponibles.\n"
                
                # Agregar datos de dolor de rodilla
                context += "\nü¶µ HISTORIAL DE DOLOR DE RODILLA:\n"
                pain_logs = conn.execute("SELECT date, level, location, notes FROM pain_logs ORDER BY created_at DESC LIMIT 5").fetchall()
                if pain_logs:
                    for p in pain_logs:
                        pain_date = p['date'].split('T')[0] if 'T' in p['date'] else p['date']
                        context += f"  ‚Ä¢ {pain_date}: Nivel {p['level']}/10 - {p['notes']}\n"
                else:
                    context += "  ‚Ä¢ No hay registros de dolor.\n"
                    
            finally:
                conn.close()
            
            # Get pain history from database
            # NEW: Get context via MCP (Standard V4 Zero-Copy)
            mcp_ctx = await self.mcp_client.get_full_coach_context()
            
            # Extract and format data for the reasoning engine
            try:
                pain_history = json.loads(mcp_ctx.get('pain_history', '[]'))
                if not isinstance(pain_history, list):
                    pain_history = []
            except:
                pain_history = []
                
            foundational = f"""
## PERFIL Y CONTEXTO DE USUARIO (MCP)
{mcp_ctx.get('user_context', 'No disponible')}

## PLAN DE ENTRENAMIENTO (Knowledge Hub)
{mcp_ctx.get('training_plan', 'No disponible')}

## MANUAL T√âCNICO DE FISIOTERAPIA
{mcp_ctx.get('manual_fisioterapia', 'No disponible')}

- Peso/Composici√≥n: {mcp_ctx.get('weight')}
- Frecuencia Card√≠aca/HRV: {mcp_ctx.get('heart_rate')}

## EQUIPAMIENTO Y OD√ìMETRO
{mcp_ctx.get('equipment', 'No disponible')}

## MANUAL MASTER 49+ (PROTOCOLO 9 D√çAS)
{mcp_ctx.get('manual_master_49', 'No disponible')}
"""
            
            # Build SYSTEM 2 CHAIN-OF-THOUGHT PROMPT
            prompt = f"""Eres el Coach de BioEngine, un entrenador experto para atletas m√°ster. 
Analiza los datos del usuario usando RAZONAMIENTO DELIBERATIVO (System 2).

**IMPORTANTE: Sigue estos pasos de pensamiento ANTES de generar tu an√°lisis:**

## PASO 1: PENSAR (An√°lisis de Datos)
Examina los datos disponibles e identifica patrones clave:
- ¬øQu√© tendencias observas en actividades, peso y dolor?
- ¬øHay se√±ales de alarma o mejoras significativas?
- ¬øLos datos son consistentes con el perfil m√©dico del usuario?

## PASO 2: VERIFICAR (Restricciones de Seguridad)
Comprueba contra estas restricciones m√©dicas CR√çTICAS:
- ‚úÖ NO recomendar ejercicios de alto impacto si dolor > 3/10
- ‚úÖ NO aumentar carga m√°s de 10% por semana (atleta m√°ster)
- ‚úÖ RESPETAR tendinosis rotuliana activa (evitar saltos, sprints en fr√≠o)
- ‚úÖ RESPETAR pronaci√≥n severa/pie plano (uso obligatorio de plantillas)
- ‚úÖ RESPETAR psoas acortado (estiramientos diarios obligatorios)
- ‚úÖ RESPETAR recuperaci√≥n m√°ster (48-72h entre sesiones del mismo grupo muscular)

## PASO 3: SIMULAR (Consecuencias)
Antes de recomendar algo, preg√∫ntate:
- ¬øQu√© pasar√≠a si el usuario sigue esta recomendaci√≥n dado su estado actual?
- ¬øHay riesgos de lesi√≥n o sobrecarga?
- ¬øEs sostenible a largo plazo?

## PASO 4: DECIDIR (Generar An√°lisis)
Bas√°ndote en los pasos anteriores, genera tu an√°lisis con este formato OBLIGATORIO:

## ‚öôÔ∏è RAZONAMIENTO DEL ENTRENADOR (System 2)
### Paso 1: Pensamiento Cr√≠tico y An√°lisis de Datos
[Razonamiento sobre tendencias de peso, dolor y entrenamiento detectados]

### Paso 2: Verificaci√≥n de Restricciones y Seguridad
[Cruzamiento de datos con historial m√©dico y reglas del manual]

### Paso 3: Simulaci√≥n de Resultados
[Evaluaci√≥n de riesgos/beneficios de las recomendaciones propuestas]

### Paso 4: Decisi√≥n y S√≠ntesis Final
[Justificaci√≥n t√©cnica de las acciones recomendadas]

---

# üèÉ‚Äç‚ôÇÔ∏è An√°lisis del Coach BioEngine

## üìä RESUMEN EJECUTIVO
[2-3 l√≠neas del estado general: ¬øEst√° progresando? ¬øHay alertas?]

## ‚öñÔ∏è PESO Y COMPOSICI√ìN
‚Ä¢ Peso actual: [X kg el DD/MM/YYYY]
‚Ä¢ Tendencia: [Mejorando/Estable/Empeorando - explicar cambio en √∫ltimas semanas]
‚Ä¢ Interpretaci√≥n: [Impacto en el rendimiento y salud articular]

## ü¶µ ESTADO DE RODILLA
‚Ä¢ √öltimo registro de dolor: [Nivel X/10 el DD/MM/YYYY - SIEMPRE mostrar el √∫ltimo registro, incluso si es 0]
‚Ä¢ Tendencia: [Mejorando/Estable/Empeorando basado en historial]
‚Ä¢ Interpretaci√≥n: [Si nivel = 0: "Excelente estado, ventana √≥ptima para progresi√≥n controlada". Si nivel > 0: an√°lisis de restricciones]
‚Ä¢ Recomendaci√≥n inmediata: [Si nivel = 0: "Aprovechar para ejercicios de Fase 2-3 del plan". Si nivel > 0: acciones espec√≠ficas seg√∫n nivel]

## üèÉ √öLTIMA ACTIVIDAD Y EVOLUCI√ìN
‚Ä¢ Actividad: [Tipo - Distancia - Duraci√≥n - Fecha]
‚Ä¢ Comparaci√≥n evolutiva: [Comparar con actividades similares previas: ¬ømejor√≥ el ritmo? ¬øaument√≥ la distancia?]
‚Ä¢ An√°lisis t√©cnico: [FC media, elevaci√≥n si aplica, cadencia]

## üí™ RECOMENDACIONES PRIORIZADAS
1. **[Acci√≥n m√°s importante]**: [Explicaci√≥n detallada con referencia al manual de entrenamiento]
2. **[Segunda acci√≥n]**: [Explicaci√≥n]
3. **[Tercera acci√≥n]**: [Explicaci√≥n]

## ‚ö†Ô∏è ALERTAS Y PRECAUCIONES
[Si hay restricciones activas o riesgos detectados, listarlos aqu√≠. Si todo est√° bien, decir "Sin alertas activas"]

---

**DATOS DISPONIBLES:**

{context}

**HISTORIAL DE DOLOR:**
{json.dumps(pain_history, indent=2, ensure_ascii=False)}

**CONTEXTO BASE (Perfil M√©dico, Equipamiento, Manual):**
{foundational[:50000]}... [Contexto completo disponible]

**AHORA GENERA TU AN√ÅLISIS SIGUIENDO LOS 4 PASOS DE RAZONAMIENTO Y EL FORMATO OBLIGATORIO.**
"""

            # Generate analysis using Gemini with System 2 reasoning
            system_instruction = """Eres un entrenador deportivo experto especializado en atletas m√°ster (49+ a√±os).
Tu prioridad es la SEGURIDAD y la prevenci√≥n de lesiones. 
Usa razonamiento deliberativo (System 2) para tomar decisiones informadas.

IMPORTANTE: Antes de generar tu an√°lisis final, PIENSA EN VOZ ALTA siguiendo los 4 pasos:
1. PENSAR: Analiza los datos y patrones. **REVISA EL OD√ìMETRO DEL EQUIPAMIENTO (especialmente la Trek FX Sport AL 3 > 2500km).**
2. VERIFICAR: Comprueba restricciones m√©dicas y **CITAR EL PROTOCOLO DE 9 D√çAS del Manual Master 49+**.
3. SIMULAR: Eval√∫a consecuencias de tus recomendaciones.
4. DECIDIR: Genera el an√°lisis final.

SIEMPRE sigue el formato de an√°lisis especificado y a√±ade una secci√≥n de 'MANTENIMIENTO DE EQUIPO' si detectas umbrales superados.
"""

            try:
                response_text = await self._generate_content_with_retry(
                    prompt=prompt,
                    system_instruction=system_instruction
                )
                
                # Cache the analysis with dynamic TTL
                ttl = 900  # 15 minutes default
                # If there's recent pain or significant changes, reduce TTL
                if pain_history and pain_history[0].get('level', 0) > 3:
                    ttl = 300  # 5 minutes if pain is high
                
                self._analysis_cache = {
                    "content": response_text,
                    "timestamp": time.time(),
                    "ttl": ttl
                }
                
                return response_text
                
            except Exception as e:
                logger.error(f"Error generating coach analysis: {e}", exc_info=True)
                return f"Error al generar an√°lisis: {str(e)}"
            finally:
                conn.close()

    async def analyze_biomechanics_video(self, video_path: str, analysis_type: str = 'gait') -> AthleteBiometrics2026:
        """
        Analiza un video de biomec√°nica usando Gemini 3 Pro (Visi√≥n Nativa).
        Ingesta directa de video aprovechando la ventana de contexto de 1M tokens.
        """
        if not self.AI_ENABLED:
            raise Exception("AI is disabled")

        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video not found: {video_path}")

        logger.info(f"Starting biomechanics video analysis: {analysis_type} for {video_path}")

        # En una implementaci√≥n real de 2026 con google-genai SDK >= 0.3.0:
        # file = self.client.files.upload(path=video_path)
        # while file.state.name == 'PROCESSING':
        #     time.sleep(2)
        #     file = self.client.files.get(name=file.name)

        prompt = ""
        if analysis_type == 'gait':
            prompt = (
                "Analiza este video de carrera (Gait Analysis). "
                "Detecta: cadencia (SPM), tipo de pisada (strike type), "
                "pronaci√≥n, y espec√≠ficamente el riesgo de valgo de rodilla (√°ngulo en grados). "
                "Responde EXCLUSIVAMENTE con un objeto JSON que siga el esquema AthleteBiometrics2026.gait."
            )
        else:
            prompt = (
                "Analiza este video de tenis (Serve Mechanics). "
                "Detecta: p√©rdida de velocidad en el servicio, tiempo de reacci√≥n y eficiencia del golpe. "
                "Eval√∫a la fatiga biomec√°nica y riesgo de lesi√≥n. "
                "Responde EXCLUSIVAMENTE con un objeto JSON que siga el esquema AthleteBiometrics2026.tennis_fatigue."
            )

        # Simulaci√≥n de llamada SOTA 2026
        try:
            # response = await self.client.aio.models.generate_content(
            #     model=self.model_name,
            #     contents=[file, prompt],
            #     config=types.GenerateContentConfig(
            #         response_mime_type='application/json',
            #         response_schema=AthleteBiometrics2026
            #     )
            # )
            # data = json.loads(response.text)
            
            # Mock de respuesta para desarrollo inicial
            logger.info("Simulating Gemini 3 Pro response for biomechanics")
            mock_data = {
                "cl√≠nical_notes": "An√°lisis preliminar detecta buena estabilidad, pero leve valgo en rodilla derecha al fatigar.",
                "next_step": "Realizar 3 series de Clamshells antes de la pr√≥xima salida."
            }
            if analysis_type == 'gait':
                mock_data["gait"] = {
                    "cadence_spm": 174,
                    "pronation_type": "Neutral",
                    "strike_type": "Midfoot",
                    "knee_valgus_assessment": {
                        "angle_degrees": 8.5,
                        "risk_level": "medium",
                        "recommendation": "Ejercicios de gl√∫teo medio (Clamshells)"
                    }
                }
            elif analysis_type == 'tennis':
                mock_data["tennis_fatigue"] = {
                    "serve_speed_loss_pct": 12.0,
                    "reaction_time_ms": 450,
                    "stroke_efficiency": 0.85,
                    "injury_warning": False,
                    "fatigue_score": 6.5
                }
            
            return AthleteBiometrics2026(**mock_data)

        except Exception as e:
            logger.error(f"Error in biomechanics video analysis: {e}")
            raise e
    async def analyze_biomechanics_hybrid(self, video_path: str, user_profile: Dict[str, Any]) -> RiskAssessment:
        """
        An√°lisis H√≠brido: MediaPipe (Visi√≥n Local) + Gemini 3 Pro (Razonamiento Cl√≠nico).
        """
        logger.info(f"Starting hybrid analysis for {video_path}")
        
        # 1. Capa de Visi√≥n Local (MediaPipe)
        json_metrics_path = self.vision_pipeline.process_video(video_path)
        with open(json_metrics_path, 'r') as f:
            metrics_data = json.load(f)
            
        # 2. Capa de Razonamiento (Gemini 3 Pro)
        prompt_template_path = os.path.join(os.path.dirname(__file__), "clinical_prompt_template.txt")
        with open(prompt_template_path, 'r', encoding='utf-8') as f:
            system_instruction = f.read()

        # Enriquecer el prompt con datos reales
        user_context_str = f"Atleta: {user_profile.get('name')}, Edad: {user_profile.get('age')}, " \
                           f"Historial: {user_profile.get('injury_history')}, Dolor Actual: {user_profile.get('pain_level')}/10"
        
        metrics_str = json.dumps(metrics_data["metrics"], indent=2)
        
        prompt = f"### DATOS DEL ATLETA:\n{user_context_str}\n\n" \
                 f"### M√âTRICAS MEDIAPIPE (3D LANDMARKS):\n{metrics_str}\n\n" \
                 f"Analiza el riesgo biomec√°nico y responde en el formato JSON solicitado."

        try:
            # Configurar respuesta estructurada (SOTA 2026)
            response = await self.client.aio.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    response_mime_type='application/json'
                )
            )
            
            raw_response = response.text
            # Safety Check: Inyecci√≥n de Alerta de Asimetr√≠a Manual si falla el LLM
            if metrics_data["metrics"]["asymmetry_pct"] > 15.0:
                logger.warning("HIGH ASYMMETRY DETECTED (>15%). Forcing alert.")
                # Aqu√≠ podr√≠amos modificar el JSON de respuesta antes de validarlo
            
            return RiskAssessment.model_validate_json(raw_response)

        except Exception as e:
            logger.error(f"Error in hybrid analysis: {e}")
            # Fallback a un objeto de riesgo conservador si falla la IA
            return RiskAssessment(
                risk_level="ALTO",
                observations=["Error en el procesamiento de IA."],
                recommendation="Detener actividad y consultar f√≠sio.",
                clinical_rationale="El sistema no pudo validar la seguridad del movimiento.",
                asymmetry_alert=metrics_data["metrics"]["asymmetry_pct"] > 15.0
            )
